\documentclass[11pt]{article}
\usepackage{fec}
\usepackage{mathtools}
\usepackage{longtable}
\usepackage{array}
\usepackage{graphicx}
\usepackage{lscape}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{hhline}
\usepackage{subfigure}

\usepackage[dvipsnames]{xcolor}
\colorlet{LightRubineRed}{RubineRed!70!}
\colorlet{Mycolor1}{red}
\definecolor{Mycolor2}{HTML}{00F9DE}

\begin{document}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert} 

\title{Complexity of Pivot Algorithms}
\author{Qi Wang, Sean Kelley}
\date{December, 2020}
\maketitle


\begin{abstract}
Abstract...
\end{abstract}


\section{Introduction}
Simplex method was invented by Dantzig in 1947 \cite{dantzig1951maximization} to solve the Linear Optimization (LP) problem. It was tableau pivoting based method and pivoting under certain rules. The rules can be flexible so many simplex variants have been developed afterwards. Another framework of pivot algorithm is criss-cross method, which was proposed by Zionts at 1969 \cite{fukuda1997criss} and then \cite{terlaky1987finite}, \cite{chang1979least} present finite criss-cross version independently around 1980s. Some variant pivot methods appeared by proposing different pivots rules. We will describe them in the following section. \textbf{Mention Short simplex paths}\\
Although simplex method was generally efficient in practical, theoretically, Klee and Minty \cite{wikipediacontributors_2020_kleeminty} in 1970s showed the worst case that, a specific type of LO problem, a variant of simplex method visited all vertices (exponential steps) until solve the problem to be optimal. The feasible region of such problems was call Klee-Minty cube whose corners have been "squashed". We will demonstrate some examples in the following section. For now, it is still an unaccomplished problem that to design a pivot algorithm and prove that the number of pivot steps is bounded by polynomial of number of variables and constraints.
The paper is structured as follows: in Section 2 we define the LO problem, list the notation and (\textbf{might present some algorithms}); in section 3 we discuss the complexity of pivot algorithms and give a brief conclusion in the final section.

\section{The pivot algorithm for LO}
In this paper, we consider the primal Linear optimization problem of the standard form
\begin{align}
\begin{split}
\min \quad &c^Tx\\
\text{s.t.} \quad &Ax = b\\
&x \ge 0\\
 x \in \Rmbb^n,\ c \in \Rmbb^n,\ &A \in \Rmbb^{m\times n},\ b \in \Rmbb^m   
\end{split} \label{LOprimal}
\end{align}
And the dual problem is 
\begin{align}
\begin{split}
\min \quad &b^Ty\\
\text{s.t.} \quad &A^y + s = c\\
&s \ge 0\\
 y \in \Rmbb^m&,\ s \in \Rmbb^n   
\end{split} \label{LOdual}
\end{align}
\begin{table}[h]
\caption{Notation}
\centering
\begin{tabular}{lll}
\hline
$n$ & number of variables   &  \\
$m$ & number of constraints (assume $m \le n$) &  \\
$B$ & set of index of basis solution $x_B$, $|B| = m$&  \\
$N$ & set of index of nonbasis solution $x_N$, $|N| = n-m$&  \\
\hline
\end{tabular}
\end{table}

\section{Complexity}
\subsection{Historical Review}
As we will go on to see in this article, much of our conversation will revolve around advancements in pivoting rules that led to performance increases for pivot algorithms. Before getting there, however, we would like to begin by discussing a few key improvements to other aspects of pivot algorithms that contributed to research focus in this space. We will start by going all the way back to the beginning.

\subsubsection{Dantzig's Simplex Algorithm}
As mentioned in our opening, the major advancement that opened the space of pivot algorithms was the development of the Primal Simplex Algorithm (referred to hereon as Simplex) by Dantzig, and his work will serve as our baseline for the complexity of pivot algorithms as we go on to compare them moving forward. In order to understand the complexity of the algorithm Dantzig proposed, let's take a moment to refresh ourselves on its operations. Although mentioned in greater detail in \cite{dantzig1951maximization}, the important details to recall are as follows. We begin by setting up the following system of equations where the equality exists between the first and second columns:
\[\begin{array}{|c|c|c|c|}
	\hline
	0 & -c_B^T x_B & -c_N^T x_N & x_0 \\
	\hline
	&&&\\
	b & A_B x_B & A_N x_N & 0 \\
	&&&\\
	\hline
\end{array}\]
Then with a couple of algebraic operations, we get the following tableau:
\[\begin{array}{|c|c|c|c|}
	\hline
	-c_B^T A_B^{-1} b & 0 & (c_B^T A_B^{-1} A_N -c_N)^T x_N & x_0 \\
	\hline
	&&&\\
	x_B & I & A_B^{-1}A_N x_N & 0 \\
	&&&\\
	\hline
\end{array}\]
%-----------revise
\textcolor{Mycolor1}{First and second columns have problem, should be: }
\[\begin{array}{|c|c|c|c|}
	\hline
	c_B^T A_B^{-1} b & 0 & (c_B^T A_B^{-1} A_N -c_N)^T x_N & x_0 \\
	\hline
	&&&\\
	A_B^{-1}b & Ix_B & A_B^{-1}A_N x_N & 0 \\
	&&&\\
	\hline
\end{array}\]
%-----------revise

Simplex tells us to then pivot through bases until we find one that is either optimal or inconsistent. The pivoting operation for primal simplex method is as following
\begin{itemize}
	\item If there is dual infeasible
	\begin{itemize}
		\item Find a index from nonbasis, which are dual infeasible, i.e, choose $q \in \Qcal$ where $\Qcal
		:= \{j \in N: (c_B^T A_B^{-1} A_j -c_j) > 0\}$. If $|\Qcal| = 1$, we just choose the only index, otherwise we make choice by some rules (we talk later). 
		\item Find a index from basis by ratio test to keep the primal solution being feasible, i.e., choose $p \in \Pcal$ where $\Pcal:=\{\arg \min_{i \in B} \frac{x_i}{(A_B^{-1}A_q)_{i}}: (A_B^{-1}A_q)_{i} >0 \}$.  If $|\Pcal| = 0$, the LO is dual infeasible and primal unbounded, else if $|\Pcal| = 1$, we choose the only index, otherwise we make choice by some rules. 
		\item Perform a pivot: $B:= B \cup \{q\} \backslash \{p\}$, $N:= N \cup \{p\} \backslash \{q\}$.
	\end{itemize}
	\item else, the current solution solves the LO problem.
\end{itemize} 
\subsubsection{Criss-cross algorithm}
The difference between the criss-cross algorithm and simplex method is that criss-cross does not maintain either primal or dual feasibility, and thus omit the ratio test procedure. The similarities is that both simplex method and criss-cross algorithm are pivot algorithms and need some rules to choose the pivot element in each iteration. Such rule is crucial for the complexity and the finiteness of the whole algorithms. 
 
\subsection{Pivoting rules}
From now on, we focus on the simplex method unless otherwise stated. In simplex method, the decision of which nonbasis enter and which basis leaves (if more than one basis satisfy ratio test), is flexible. Thus many pivot rules are designed and we show some of them. 
\subsubsection{Dantzig's rule}
To make a pivot, Dantzig said to do the following:
\begin{itemize}
	\item Find the most positive element in the top row. Denote its index as $q$ ($n-m$ operations)
	\item Find the smallest $x_i/ \tau_{iq} : \tau_{iq} > 0$, where $\tau_{iq}$ is the $i$th element of the $q$th column in our tableau. If there is a tie, arbitrarily pick one of the tying elements ($3m$ operations)
	\item Swap $i$ and $q$ in their respective index sets.
	\item Multiply $A_B^{-1}$ by $A_N$ ($m^2*(n-m)$ operations)
	\item Add $c_B^T A_B^{-1} A_N$ to $-c_N^T$ ($m^2 + m(n-m) + (n-m)$ operations)
\end{itemize}
We see then this takes us to a total of $m^2 (n - m) + m^2 + m (n - m) + m + 2 n = O(m^2n)$ operations for the pivot.

Now as we showed in class \cite{class}, we know that so long as there is no degeneracy in the problem, simplex cannot cycle, so the number of pivots is bounded by the number of possible bases, which was $\binom{n}{m}$. Therefore, we know Dantzig's version of simplex to have complexity $O(\binom{n}{m}m^2n)$ for nondegenerate problems.

\subsubsection{Bland's Rule (Least-index)}
Now for Dantzig's simplex, we assumed that a problem would have no degeneracy, which is a rather high standard for using the algorithm in practice. To further complicate the matter, degeneracy leads to cycling in that version of simplex, which means the algorithm could run infinitely and never converge. To get around that issue then, many pivot rules have been proposed which help simplex avoid cycles. Arguably none of these such pivoting rules are more famous than Bland's Rule, which updates the above steps of simplex in the following way:
\begin{itemize}
	\item instead of picking the most positive top row element to enter the basis, pick the first positive top row element.
	\item instead of arbitrarily selecting a basis row matching the minimal ratio, select the minimal ratio basis row with least index.
\end{itemize}
As shown in \cite{bland}, these updates to Dantzig's simplex method ensure that we never cycle and make all linear programs converge and finite. Thus all linear programs now have complexity $O(\binom{n}{m}m^2n)$.

\subsubsection{Revised Simplex Method}
With simplex having a finite worst case across all problems, it is worthwhile to look at ways now to lower that upper bound. One of the first places the optimization community had success in doing this was with speeding up the process of making a pivot, which has come to be known as the Revised Simplex Method. Again, Revised Simplex differs from Dantzig's Simplex in two key ways \cite{revised}:
\begin{itemize}
	\item instead of calculating all of the objective row for each pivot, just calculate enough columns until we get a positive entry, pivoting on that (up to $n-m$ operations)
	\item instead of multiplying $A_B^{-1}$ by all of $A_N$, just multiply by the column corresponding to our first found positive entry ($m^2$ operations).
\end{itemize}
It is worth highlighting at this point that \cite{revised} shows Revised Simplex yields the same solution as Dantzig's Simplex but saves us from making many unnecessary calculations. This change brings our pivot to $2 m^2 + m (n - m) + m + 2 n = O(mn)$ operations (the same as in Dantzig's Simplex but swap $m^2(n-m)$ for $m^2$).


\subsubsection{Best improvement rule}

Also called largest improvement rule and greatest improvement rule. It aims for largest increase in objective value. It will pick the nonbasic variable with the $j := \arg \max_{j\in N}(c_B^T A_B^{-1} A_j - c_j)\times t_j$ where $t_j = \min_{i \in \{B:(A_B^{-1}A_j)_{i} >0\}} \frac{x_i}{(A_B^{-1}A_j)_{i}}$. In other words, the improvement to objective value is the reduced cost times the ratio. And we choose nonbasis that bring largest improvement in this iteration. For example, as in \cite{largest}, the tableau is
\begin{table}[h]
\centering
\begin{tabular}{cccccc}
                        & rhs                    & $-s_1$ & $-s_2$ & $-s_3$ & $-s_4$                    \\ \cline{2-6} 
\multicolumn{1}{l|}{$z$}  & \multicolumn{1}{l|}{0} & 2   & 3   & 0   & \multicolumn{1}{l|}{0} \\ \cline{2-6} 
\multicolumn{1}{l|}{$x_3$} & \multicolumn{1}{l|}{2} & 1   & 2   & 1   & \multicolumn{1}{l|}{0} \\
\multicolumn{1}{l|}{$x_4$} & \multicolumn{1}{l|}{3} & 1   & -1  & 0   & \multicolumn{1}{l|}{1} \\ \cline{2-6} 
\end{tabular}
\end{table}

Now $x_1, x_2$ are nonbasis that are dual infeasible. For $x_1$ we can increase it to $t_1 = \min \{\frac{2}{1}, \frac{3}{1}\} = 2$; for $x_2$, $t_2 = \min \{\frac{2}{2}\} = 1$. Since $2t_1 = 4 > 3 = 3t_2$, we choose $x_1$ to enter according to this rule.

Apparently, this rule need more arithmetic operations in each iteration. But it may save number of iterations, which we will talk later.

\subsubsection{Steepest edge rule}
Similar to Dantzig's rule, but instead of choose the most positive element in the top row, we choose $\max_{j \in N} \frac{c_B^TA_B^{-1}A_j - c_j}{\norm{A_B^{-1}A_j}}$ (normalize reduced cost with the norm of its column).

\subsubsection{Last-in-first-out rule (LIFO)}
Apart from Bland's rule, this last-in-first-out, plus the following most-often-selected-variable (MOSV) rule are also finite pivot rules. LIFO and MOSV are proposed by Zhang in 1997 \cite{zhang1999new}. LIFO means among the candidates, choose the most recently moved variable. In the first iteration with no selection in the past, we use Bland's rule or Dantzig's rule.

\subsubsection{Most-often-selected-variable (MOSV)}
MOSV rule is to select the variable that has been selected the largest amount of times before, that is the "most often selected". In the first iteration with no selection in the past, we use Bland's rule or Dantzig's rule.

Also, when choice by LIFO (or MOSV) rule is not unique, we use Dantzig's rule. This called Hybrid-LIFO (Hybrid-MOSV) \cite{zhang1999new}.

\subsection{S-monotone index selection rules}
Csizmadia and Illés in 2010 \cite{csizmadia2012s} proposed that Bland, LIFO, MOSV pivot rules are s-monotone index selection rules. They provide a explicit definition of s-monotone index selection rules in \cite{csizmadia2012s}. The key steps of s-monotone rule are
\begin{enumerate}
\item Maintain a sequence of vectors $s_k \in \Ncal^n$, where $k$ is the iteration number. 
\item At iteration $k$, when selecting index from a couple of candidates (tie in entering or leaving), select the index $j := \arg \max_j s_k^j$, where $j$ is the index of $s_k$.
\item When finishing pivoting, generate $s_{k+1}$ by $s_{k}$ and specific rules for Bland, LIFO and MOSV (we talk latter). 
\item $s_k$ is an nondecreasing vector from $k=0,1,2,\cdots$.
\end{enumerate}
We define $i_k, o_k$ are indices of entering variable and leaving variable at iteration $k$. The $s_k$ for Bland, LIFO, MOSV is generated as follows

\begin{itemize}
\item \textbf{Bland}. $s_k = (n, n-1, \cdots, 1)^T$ for all $k$.
\item \textbf{LIFO}. \begin{align*}
s_{k+1}^i = \left\{
\begin{aligned}
&k, & \text{if } i \in \{i_k, o_k\},\\
&s_k^i & \text{ otherwise}. 
\end{aligned}
\right.
\end{align*}
\item \textbf{MOSV}. \begin{align*}
s_{k+1}^i = \left\{
\begin{aligned}
&s_k^{i} + 1, & \text{if } i \in \{i_k, o_k\},\\
&s_k^i & \text{ otherwise}. 
\end{aligned}
\right.
\end{align*}

\end{itemize}
Csizmadia and Illés \cite{csizmadia2012s} then proved simplex method (as well as criss-cross) with s-monotone rule is finite algorithm. They also provided guidance of revising other pivot rules to be finite. For example, if we want to apply steepest edge rule but want to be finite, we can make some modification that, 
\begin{enumerate}
\item At the beginning, define a sequence $\{p_k\}$ that is strictly increasing.
\item At iteration $k$, let
$$
\gamma = \max_{j \in N} \frac{c_{B^k}^TA_{B^k}^{-1}A_j - c_j}{\norm{A_{B^k}^{-1}A_j}},
$$
and adjust $p_k$ by
 \begin{align*}
p_k = \left\{
\begin{aligned}
&p_k + \delta, & \text{if } p_{k-1} \ge \gamma,\\
&\gamma, & \text{ otherwise}. 
\end{aligned}
\right.
\end{align*}
where $\delta > 0$ is a given number. The $\{p_k\}$ by such adjustment is strictly increasing. Therefore we can use it to update the $s_{k}$ as follows (similar to LIFO)
\begin{align*}
s_{k}^i = \left\{
\begin{aligned}
&p_k, & \text{if } i \in \{i_k, o_k\},\\
&s_{k-1}^i & \text{ otherwise}. 
\end{aligned}
\right.
\end{align*}
\item Select the candidates (of indices) that has largest value of $s_k$.
\end{enumerate}
In such way the steepest edge method is combined with LIFO, and is s-monotone index rules (finite). It can also be combined with MOSV similarly. Consequently, they state an possible research direction is that to analyze the complexity of s-monotone index selection rules.

\section{Number of Iterations}
\subsection{When will worst case happen}
Given $m$ constraints and $n$ variables, there are at most $\binom{n}{m}$ possible basis, which is the biggest upper bound for the number of iterations for pivot algorithms. When $m=\frac{n}{2}$, $\binom{n}{m}$ get its maximum $\binom{n}{\frac{n}{2}}\approx \sqrt{\frac{2}{\pi n}}2^n$. Klee and Minty defined a type of LO problems which some pivot algorithms may visit all vertices before finally finding the optimal solution. Detailed introduction of Klee Minty cube can be referred to \cite{vanderbei2020linear}. Here we demonstrate some examples. Klee Minty cube is defined as following LO problem \ref{eq:klLO}. The coefficients might be different in different articles. 
\begin{align}
\begin{split}
\max &\sum_{j=1}^n 2^{n-j}x_j \\
\text{s.t. } & 2\sum_{j=1}^{i-1}2^{i-j}x_j + x_i \le 5^i, \quad i=1,\cdots,n\\
			&x_j \ge 0,  \quad j=1,\cdots,n
\end{split}\label{eq:klLO}
\end{align} 
When $n=2$ and $n=3$ the 2-d and 3-d Klee Minty LO problems are 
\begin{align*}
\begin{split}
\max \quad & 2x_1 + x_2 \\
\text{s.t. }\quad  & x_1 \le 5\\
& 4x_1 + x_2 \le 25\\
& x_1 \ge 0,\ x_2 \ge 0.
\end{split}
\begin{split}
\max \quad & 4x_1 + 2x_2 + x_3 \\
\text{s.t. }\quad  & x_1 \le 5\\
& 4x_1 + x_2 \le 25\\
& 8x_1 + 4x_2 + x_3 \le 125\\
& x_1 \ge 0,\ x_2 \ge 0, \ x_3 \ge 0.
\end{split}
\end{align*}
The feasible region of 2-d and 3-d Klee Minty problems are shown in Figure \ref{fig:kmcube}. They look like "squashed" square and cube, which are very ill-conditioned.
\begin{figure}
    \centering
    \subfigure[]{\includegraphics[width=0.38\textwidth]{klee_cube_d2.png}}
    \subfigure[]{\includegraphics[width=0.59\textwidth]{klee_cube_d3.png}}
    \caption{(a) Feasible region of 2-d Klee Minty Cube with contour lines (b) Feasible region of 3-d Klee Minty Cube}
    \label{fig:kmcube}
\end{figure}
We apply primal simplex method (a) least-index and (b) Dantzig pivot rules to demonstrate the number of pivot steps. The tableau result of least-index pivot rule is shown in Table \ref{Table: tableau}. The results of Dantzig pivot rule and least-index rule just coincide in this example because in first iteration, the nonbasis with the most negative reduced cost is $x_1$ which is the entering nonbasis by least-index rule, too. 

\begin{table}[H]
\caption{Tableau of primal simplex method (least index version), the element with * is the pivot element. }
\label{Table: tableau}
\centering
\begin{tabular}{c|llllll}
   & rhs & $x_1$ & $x_2$ & $s_1$ & $s_2$ &                 \\ \cline{1-6}
$z$  & 0   & 2  & 1  & 0  & 0  & at vetex (0,0)  \\
$s_1$ & 5   & $1^*$  & 0  & 1  & 0  &                 \\
$s_2$ & 25  & 4  & 1  & 0  & 1  &                 \\ \hhline{======}
$z$  & -10 & 0  & 1  & -2 & 0  & at vetex (5,0)  \\
$x_1$ & 5   & 1  & 0  & 1  & 0  &                 \\
$s_2$ & 5   & 0  & $1^*$  & -4 & 1  &                 \\ \hhline{======}
$z$  & -15 & 0  & 0  & 2  & -1 & at vetex (5,5)  \\
$x_1$ & 5   & 1  & 0  & $1^*$  & 0  &                 \\
$x_2$ & 5   & 0  & 1  & -4 & 1  &                 \\ \hhline{======}
$z$  & -25 & -2 & 0  & 0  & -1 & at vetex (0,25) \\
$s_1$ & 5   & 1  & 0  & 1  & 0  &                 \\
$x_2$ & 25  & 4  & 1  & 0  & 1  &                 \\ \hhline{======}
\end{tabular}
\end{table}
By taking a look on the constraints, the first constraint says that $x_1$ is no bigger than 5. With this in mind, the second constraint says that $x_2$ has an upper bound of about 25, depending on how big $x_1$ is. Similarly, the third constraint says that $x_3$ is roughly no bigger than 125 (again, this statement needs some adjustment depending on the sizes of $x_1$ and $x_2$). Therefore, the constraints are approximately just a set of upper bounds, which means that the feasible region is virtually a stretched n-dimensional hypercube, which has $2^n$ vertices. And Klee and Minty \cite{klee1972good} showed that Dantzig pivot rule with primal simplex algorithm will conduct $2^n -1$ pivot steps.

\subsection{Kitahara and Mizuno analysis}
In 2011, Kitahara and Mizuno \cite{kitahara2013bound} proved the complexity can be bounded by a polynomial of $m, n$ and a special ratio. They utilized the analysis of Ye (2010) \cite{ye2010simplex} for Markov Decision Problem. We will summarize the key results of Kitahara and Mizuno \cite{kitahara2013bound} in this section. 

We define some new notations. A BFS is said to be a basic feasible solution for primal LO problem (\ref{LOprimal}). Let $\delta$ and $\gamma$ be the minimum and the maximum values of all the positive elements of all BFSs. That is, for any BFS $\hat{x} \in \Rmbb^{n}$, if $\hat{x}_j \neq 0, \ j \in \{1, \cdots, n\}$ where the subscript $j$ represent the $j$th entry of $\hat{x}$, we have 
\begin{align}
\delta \le \hat{x}_j \le \gamma. \label{eq:bfsbound}
\end{align} 

The most important result of Kitahara and Mizuno \cite{kitahara2013bound} is the following theorem and corollary.
\begin{theorem}
When we apply the simplex method with the Dantzig's rule or the best improvement rule for LO \ref{LOprimal} having optimal solutions, we encounter at most
\begin{align}
n\lceil m \frac{\gamma}{\delta}\log(m\frac{\gamma}{\delta})\rceil \label{kmcomplexity}
\end{align}
different basic feasible solutions. \label{KTheorem1}
\end{theorem}

\begin{corollary}
 If the primal problem is nondegenerate, the simplex method finds an optimal solution in at most $n\lceil m \frac{\gamma}{\delta}\log(m\frac{\gamma}{\delta})\rceil$ iterations.
\end{corollary}
The proof can be referred to Kitahara and Mizuno \cite{kitahara2013bound}. 

Therefore the bound (\ref{kmcomplexity}) is a polynomial of $n, m$ and $\frac{\gamma}{\delta}$ with assumptions that the primal LO \ref{LOprimal} has optimal solutions and is nondegenerate. In practical, the ratio of $\frac{\gamma}{\delta}$ is not easy to get prior to solving the problem. However, for some specific form of LO, $\frac{\gamma}{\delta}$ can be bounded using  LO coefficients. 
\subsubsection{LO with a totally unimodular matrix A and integeral b}
\begin{definition}
A matrix A is totally unimodular if every square submatrix has determinant 0, -1 or 1. In particular, this implies that all entries are 0, -1 or 1.
\end{definition}
Totally unimodular matrices are very well behaved, because they always define polytopes with integer vertices, as long as the right-hand side is integer-valued. Many practical problem can be modelled using totally unimodular matrices, such as the node arc incidence matrix of a directed graph.

With totally unimodular $A$ and integral $b$, all the elements of any BFS are integers, so $\delta \ge 1$. And for a BFS $x = (x_B, x_N)$, $x_N = 0$ and $x_B = A_B^{-1}b \ge 0$, since $A_B$ is totally unimodular, $A_B^{-1}$ is unimodular \cite{kitahara2013bound} and all the elements of $A_B^{-1}$ are $-1, 0$ or $1$. Thus for any $j \in B$ we have $x_j \le \norm{b}_1$, which implies $\gamma \le \norm{b}_1$. Thus $\frac{\gamma}{\delta} \le \frac{\norm{b}_1}{1}$. Thus by Theorem \ref{KTheorem1}, we have
\begin{corollary}
Assume that the constraint matrix A of LO (\ref{LOprimal}) is totally unimodular and the constraint vector b is integral. When we apply the simplex method with the Dantzig's rule or the best improvement rule for LO (\ref{LOprimal}), we encounter at most $n\lceil m \norm{b}_1\log(m\norm{b}_1)\rceil$ different basic feasible solutions. Moreover if the LO (\ref{LOprimal}) is nondegenerate, this is the most iterations to find optimal solution. 
\end{corollary}

\subsubsection{Markov decision problem}
In \cite{kitahara2013bound}, Kitahara and Mizuno also extended the theory for Markov decision problem (MDP), where the number of possible actions is two, is formulated as
\begin{align}
\begin{split}
\min \quad &c_1^Tx_1 + c_2^Tx_2 \\
\text{s.t. } \quad &(I-\theta P_1)x_1 + (I - \theta P_2)x_2 = e, \\
&x_1, x_2 \ge 0
\end{split}
\end{align}
where $I$ is the $m \times m$ identity matrix, $P_1, P_2$ are $m\times m$ Markov matrices (Markov transition matrices), $\theta$ is a discount rate, and $e$ is the vector of all ones. MDP has following properties.
\begin{enumerate}
\item MDP is nondegenerate.
\item The minimum value of all the positive elements of BFSs is greater than or equal to 1, i.e., $\delta \ge 1$.
\item he maximum value of all the positive elements of BFSs is less than or equal to $\frac{m}{1-\theta}$, i.e., $\gamma \le \frac{m}{1-\theta}$
\end{enumerate}
Therefore, again by Theorem \ref{KTheorem1}, Kitahara and Mizuno \cite{kitahara2013bound} obtained the similar result to Ye \cite{ye2010simplex}
\begin{corollary}
 The simplex method for solving MDP finds an optimal solution in at most $\lceil m \frac{m^2}{1-\theta}\log(\frac{m^2}{1-\theta})\rceil$ iterations, where $n=2m$.
\end{corollary}

%\section{title}
%There are certain pivot rules:
%\begin{itemize}
%\item Bland's rule, lexicographic rule
%\item Clairvoyant pivot rule (only theoretical), unimplementable
%\item Hirsch conjecture : disproven by Francisco Santos, 2011
%\item Polynomial Hirsch conjecture -- still open
%\end{itemize}


\section{Conclusion}
Conclusion...

\bibliographystyle{plainurl}
\bibliography{reference.bib} 

\section{Outline Suggestion}
what are pivot algorithms?
include point that there are two parts - selecting the pivot and making the pivot

Why did pivot algorithms come about?

what kinds are there?
simplex
criss cross

who developed them? when?

how do they differ (include problem assumptions/set up and iteration differences)?

what were their pivot calculation (everything) and pivot choosing rules (e.g. dantzigs rule - just most negative reduced cost/element in objective row)

what were their complexity?
O(something) when nondegenerate in worst case
degeneracy could still create infiniteness/nonconvergence

what is degeneracy?

why does degeneracy matter (how does it affect complexity)?

how did we get around degeneracy?
zadeh's/cunningham's rules
blands rule

What did this bring complexity to?
same as before, now just covering a much wider array of problems
now all were finite/convergent

How could we improve this?
either improve the pivot operation or the pivot selection

How could we improve the pivot operations?
Revised simplex method

What is the revised simplex method? Who worked on this and when?

What did this bring complexity to?

This became the basis for modern solvers using pivot algorithms (get confirmed by gurobi/cplex/xpress/coin)
mention improvements made by preprocessing, but don't go into detail - not sure if they are problem specific or general to pivot algorithms

Why worry about improving pivot selection?
pivot algorithms usually average polynomial time complexity
but klee-minty showed that worst case complexity can actually be achieved
(what is klee-minty)

What were some early "improvements" to pivot selection?
Greatest Improvement (calculates which column reduces objective the most)
Steepest edge (normalize reduced cost with the norm of its column)
(I think these were actually slower in the average case because when you have a lot of variables the quick calculations on bland's rule win out)

Ok this is where I am stuck - these things cover us nicely until 1980's and our papers pick up 2010's. I feel like we should have something to fit the in between in terms of advancement of computational complexity of pivot algorithms


\end{document}
